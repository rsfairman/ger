
% Updated discussion of ways to go about writing the G-code simulator.

\documentclass[titlepage,oneside,10pt]{article}

% Need this for pmatrix (among other things).
\usepackage{amsmath}
\usepackage{amsfonts}

% Needed to include graphics objects (from metapost).
\usepackage{graphics}

\usepackage{epic,eepic} \setlength\maxovaldiam{60pt}

% This is so that \atan2 is printed in roman type for formulas, like
% \cos, \sin, etc. Taken from the original TeX book, p. 361.
\def\atan2{\mathop{\rm atan2}\nolimits}

\begin{document}

\raggedbottom

% These are for marginal notes on the right or left of the main text.
\newcommand{\mymargin}[1]{\marginpar{\rm\tiny #1}}
%\newcommand{\mymargin}[1]{\marginpar{\rm\scriptsize #1}}
\newcommand{\leftmar}[1]{\reversemarginpar \mymargin{#1}}

I am looking at writing a G-code simulator again. There are two other
documents that have a lot more background, but I want a fresh
start. These two documents are found in the {\tt old vcnc} directory. One is an
overview of differnt strategies that I have considered and the other
is a more detailed look at the steps along the way as I tried
different ideas. These are in {\tt notes.tex} and {\tt
  overview.tex}. There is also {\tt manual.tex}.

\section{April 18, 2013}

I'm looking at this again for a couple of reasons. One is that I want
to learn Qt and refresh my C++ fluency, and this is a relatively easy
program to do that. Not only is this a good way to learn (or relearn)
this stuff, but I think I could sell this program over the internet --
not many copies, but enough to make it worth figuring out how to set
up a website, PayPal, and so forth.

My goals are modest for the program. The resulting object may not have
any undercutting, the mill can only be a three-axis machine, and the
tools must be convex. This simplifies things a great deal.

\subsection{The Dream Program}

Someday I would like a version of this program without any
restrictions at all, but it's too difficult for me to think about
right now. Multi-axis machines, undercutting, concave tools, {\emph
  etc}. would all be nice, but they're not easy to deal with. If I do
eventually go that route, I now think that working in polygons
(triangulating the surface) is the way to go. Here are some thoughts
about that.

Dividing the volume into voxels of $0.001$ requires far too much
memory, and I suspect that the amount of processing required would be
too great in any case. Octrees are another route, but they take a lot
of memory too if they have the potential to get down to $0.001$. Also,
things like shading the surface would be a royal pain. I looked at
these issues in an earlier document. At the time, they seemed
surmountable, but I'm less sure of that now.

Something like triangulating the surface now seems to me to be the way
to go -- memory becomes a non-issue and shading the surface is almost
trivial -- but I think that it would be too slow if done naively. This approach
requires three basic operations: union, intersection and difference of
two volumes. Actually, I think I could finesse the need for union and
intersection, or at least they wouldn't be needed in their full
generality, but the difference operation is the crux of the entire
problem. See below: the interesection operation is needed, but it may
not be so hard.

I haven't done any research yet, but it seems that everything will
need to be based on convex volumes since my intuition is that a
point-in-volume routine will always come down to whether a point is in
a {\emph convex} volume, and this would be done by checking whether the
point is on the in-side of every surface polygon that bounds the
volume. 

The basic problem is how to take a given a convex volume, $V$, and
subtract from it another convex volume, $D$, to obtain $V-D$. I would
start by checking which pgons of $D$ lie entirely within $V$, which are
entirely outside, and which are partly inside and partly outside. From
this you could build a surface that is equal to $V\cap D$, and we
now want $A = V - (V\cap D) = V-D$. Now I need to
know which pgons of $V$ are inside, outside or partway in/out of 
$V\cap D$. The pgons that bound $A$ will be those of $V$ that do not
touch $D$, plus those of $D$ that are entirely inside $V$, plus a set of
pgons needed to "stitch things together." The later set of pgons is
obtained by looking at how $D$ "cuts through" the pgons of $V$ that are
not entirely inside or outside of $D$.

Once we have the bounding pgons for $V-D$, as desribed above, the volume
must be broken into pieces, where each piece is convex. This is needed
becuase I think that working with convex volumes only is what makes
the process above work in successive steps.

All of the above seems conceptually do-able -- not trivial, but probably
well-understood. However, I will need to be careful that the number of
pgons doesn't explode. Suppose that we work with triangles alone
(3gons). Each 3gon will take no more than 24 bytes -- call it $2^5 = 32$
to make it easier. Then we could store 8 million 3gons in 256 meg of
memory. That's enough (I think) for any reasonable surface, provided
that the pgons are made as "large as possible." The problem is that it
would be slow, both in terms of simple calculations, like
point-in-volume, and in terms of rendering.

One way to speed this up would be a very coarse ``voxeling'' above the
level of triangulation/p-gon-ification. If the volume is divided into
voxels of 1/16 inch, then a 10 x 10 x 10 inch cube would require
$10\cdot10\cdot10\cdot 16\cdot16\cdot16 ~ 2^{10}\cdot2^{12},$ or 4 million
elements. Each element would be either solid, empty or partial. It
would be simple (and fast) to determine whether a given point was in a
solid, empty or partial voxel. The solid and empty cases are then
finished and dealing with the partial voxel case would be easy too
since these voxels are only 64 (1024/16) to a side, and could not
involve very many pgons.

This has the additional advantage that all p-gon calculations would be
in a very restricted space. This may not matter, but it's tempting to
think that it would simplify the algorithms too.

\subsection{Back to Reality}

The dream program would be nice, but I want to focus on what I can do
more easily. The best and simplest strategy in the situation where
there is no undercutting seems to be what I called a DA (depth array)
in earlier discussions. This is a 2-d array of depth values, with one
entry for each $0.001$ inch square. Each square inch of surface will
then require 1M entries, and at least two bytes will be needed for
each entry. Something like a 10 inch square sheet is then well within
reason, using 200M (at two bytes per entry). For larger objects, I
could implement the ``quad tree with depth'' (QTD) as I did
earlier. For things that are highly detailed, like milling out a
person's face, there is no advantage to a QTD since almost every array
element will be at a different depth, but for very large objects that
are cut more simply, there is a huge memory savings.

For now, work strictly with a DA. Earlier, to save memory, I used
shorts to specify the depth, but I will use floats now, which take
four bytes rather than two. The reason is that it will be easier to
determine the surface normal if the depth value is highly accurate to
within a fraction of a voxel. On the other hand, with two bytes, I
still have 64 inches of potential depth variation, but no actual
object will every have more than a few inches of difference between
the highest and lowest point. So, each whole number could represent a
tenth of 0.001.

The reason I care about this is that I will want surface normals when
rendering, and the more accruate the depth value, the less concerned I
will need to be with weird artifacts near edges, or anywhere that the
slope changes.

Another simplification compared to what I was trying to do before is
that I will not work strictly from a pulse train. In earlier versions
I converted the G-code to a series of pulses in each direction, and
worked ``pulse by pulse,'' without any knowledge of the G-code that
generated these pulses. That is clean, but less efficient. Instead,
I will now work in terms of lines of G-code.

For the current version, I will not worry about the niceties of the
user interface, or anything close to ``niceties.'' The goal is to get
the algorithmic guts working, and then migrate the whole thing to Qt.

\subsection{Version 01A}

The ``A'' is because I'm in a ``new series'' now that I am leaving
behind a lot of the older code.

One large difference is the fact that I am no longer working with
pulses. Previously, the G-code was converted to individual pulses to
the X, Y or Z axis, and each of these pulses was handled one at a
time. The new version now starts the rendering process with the most
elementary G-code statements possible: linear moves and arc moves.

The process of generating the DA has several steps. First, we obtain
a line of G-code that specifies a cut. Next, this is converted to an
intermediate form which will be used to adjust the global DA. There
are various possibilities for this intermediate form -- pulses is one
of them, but these are not very efficient. A mini-DA based on scan
lines seems like a good choice. The idea is to create a set of scan
lines, one every 0.001 inch, where the depth of cut is given at every
point along the line -- i.e., a depth is given for every 0.001
inch. For a square-ended endmill, there is only a single depth, but a
ballmills and drills are different.

The data for these scan lines should take the form of a count of the
number of scan lines needed, the y-coordinate of the first line, and
the x-position for the start of each line and a count of the number of
steps to be taken in the x-direction, along with an array of depths
for this many steps. One tricky thing here is that the scan lines may
not have a ``gap.'' For instance a semi-circle that extends ``upward''
will need to be broken into two parts.

Once we have the scan line-based DA, we can fold it into the global DA
by simply stepping through each point.

After further thought, and looking at the original code, I see that
rendering a DA is not so easy. In particular, see the various {\tt
  paintComponent()} methods in {\tt Cut3DPane}. Earlier, I tried two
strategies, both of which occurred to me again, before I looked at the
code. One strategy is to walk along horizontal or vertical lines of
the DA and ``connect the dots.'' Provided that these lines are close
enough together, the rendering will be solid, with no gaps between the
lines. This works, but it's difficult to avoid odd artifacts and
methods of shading will always be somewhat ad hoc. The other method is
to triangulate (or use other polygons) the surface.

Both methods require walking lines of the DA, including the need to
walk up and down through depressions. Also, they both require a
z-buffer to deal with hidden surface removal. Actually, I think I
could use BSP trees (or some other method) with the polygons, but that
would probably be slower than a z-buffer, and certainly more complicated.

\section{April 20, 2013}

I started implementing some of the ideas above, in particular the idea
of encoding each cut as a collection of scanlines so that I could then
copy these to the global Depth Array. This could be done relatively
easily (although it's verbose) for endmills, but it becomes more
difficult for things like ballmills and drills because the depth is
not uniform.

I now think that a smarter version of the signal-based approach, which
I was already using, is what is needed. There are three cases, one
case for each of the possible tools: endmill (flat bottom), ballmill
(hemispherical bottom) and drill (angular bottom). 

For the endmill, start by ``drawing'' (i.e., setting the depth in the
DA) a complete solid circle where the cut starts. Then, for each
additional step, draw only the one pixel wide ``edge'' of the circle
until the tool reaches the final position. This may or may not be
faster than what I started to do with scanlines. The downside of the
scanline method is that the depths must be set twice -- once in the
scanline representation and once in the global DA. However, the new
method that I just outlined is slightly more complicated and may be a
tad slower. It wouldn't be hard to change to one method or the other
later for this single case.

For ballmills, the strategy starts by drawing only the first half of
the cut at the tool's initial position. Then, with each additional
step a one-pixel wide semicircular cut is made, taken through the
center of the tool. At the final tool position, the other half of the
ballmill's base is used to cut.

Drills and other angled cutters are handled similarly to the
ballmills. In fact, there is a similarity among all three cases. Store
a mini-DA for each tool representing the profile of the base of the
tool. At the tool's initial position, we cut by copying half of this
mini-DA to the global DA. This brings us up to a ``center line'' of
the mini-DA that represents the cuts of each additional step. Whatever
the type of tool, we need to copy this center line repeatedly through
the intermediate steps. At the final tool position, we copy the
remaining half of the mini-DA.

As noted, the advantage of this method for endmills making linear cuts
may be small or non-existent, but, even for endmills, this method is
better for cuts along arcs -- it's certainly simpler. In any case, the
big advantage of this method is that every tool is handled the same
way once you have the profile of the bottom of the tool.

Even so, cutting along arcs is not trivial. Linear cuts {\emph are}
very easy since the same one pixel wide ``slice'' through the center
of the tool simply needs to be replicaed as the tool steps along the
line. For arcs, we need a small pie-slice of the tool to be replicated
as the tool moves along the arc. Think of it this way: the outside
edge of the tool must move faster than the inside edge...On the other
hand, each step is in either the x-direction or the
y-direction. Either way, we merely copy a single slice through the
diameter. I think that the problem with these pie-shaped slices goes
away.

On second thought, this won't work...With ballmills and drills, at
each step forward, you must draw the entire half of the tool that's on
the leading edge. So, you get some savings (about half) over copying
the entire circle with each step, but not a tremendous amount. These
ideas would work for endmills though.

The way to get efficiency seems to be to handle each cut (line of
G-code) as a whole. A linear cut with a ball mill would be handled by
cutting half of the circle at each end of the path, then a series of
lines at various depths from one end of the cut to the other. Arcs
would be handled similarly, but somewhat more complicated since the
series of ``furrows'' are themselves arc of various radii.

So, it seems that what I have now, already written in Java, is not so
different from what I am aiming for. It makes sense to move away from
the Java version and start porting it to C++ and Qt. The next few
steps should be as follows: (1) write a Qt program that allows me to
open and view a text file; (2) put a splitter in the window and show
something like a 3D cube in that window with the ability to maniuplate
it with the keyboard; (3) port the G-code interpreter; (4) add various
dialogs to the program that are needed, like to specify the tool
turret; (5) move to the completed program.

\section{April 23, 2013}

I am working with Qt and liking it, although I haven't gotten very far
yet.

I had what may be a GOOD IDEA for the problem of dealing with more
general surfaces involving undercuts and the like. Triangulation is
one solution, but it may be too cumbersome, with too many triangles to
render in real time, and I'm not sure that it will look as nice as I
would like. But trying to work with something more analytic, like
NURBS, is difficult because of the problem of dealing with
intersections. There's no (computationally reasonable) way to
determine the intersection of two volumes. Even something like a
point-in-volume method is hard when the volume is bounded by NURBS. 

A hybrid method may work. Use some voxel-based method to determine
whether a particular cut has any effect; that way there's no need for
point-in-volume calcuations involving NURBS. But, whenever a voxel is cut
away, revealing some new surface voxels, the newly exposed surface
voxels should have a pointer stored with them that points to some
analytic/NURBS-like description of the larger surface of which that
voxel is a part -- in fact, each exposed face of the voxel would need
such a pointer.

After all of the cuts have been made, look at every exposed voxel and
make a list of the surface elements that appear on the
surface. Now draw them, most likely using a z-buffer for hidden
surface removal. 

For example, a simple cut with an endmill leads to a nice surface that
would be easy to define and draw. The problem is that once a
particular suface is in place, further cuts could ``mess it up.''
Suppose that a ballmill cuts a trough, then a smaller endmill cuts a
groove down the middle of the trough. The surface due to the ball mill
is nice, but the cut by the endmill means that some of that surface is
no longer there; it's been cut away by the endmill. So, what you need
are surfaces with holes and/or edges that have been cut back.

This might not be so hard to manage. It's clear how to deal with the
data structures: each surface description needs an array of pointers
to surfaces that are to be removed -- areas of the surface that are
holes. If we think of each patch as being a map $R^2\rightarrow R^3$,
then the hole could be a map $[0,1]^2\rightarrow [0,1]^2$, where the range
here is the same as the domain of the patch map. Put another way, the
holes are simply regions of the plane.

One way to draw these ``patches with holes'' is to use a second
buffer. Draw the complete (no holes) surface to the buffer, then draw
the holes, but where the holes are ``drawn'' by marking the points of
the original surface with a special code meaning ``not to be
transferred.'' Now copy from this buffer(but not the holes) to the
z-buffer that will eventually be copied to the screen. 

Another way, but this seems more complicated, is to draw only the
surface and forget about trying to keep track of holes. The holes are
made to appear as each surface is drawn by checking the voxels behind
the surface. A particular pixel of the surface either ``rests
against'' a voxel on the surface of the completed shape, or the voxel
behind that point has been cut away. Don't draw pixels corresponding
to voxels that have been cut away. This seems tricky, but might be
doable. Each pixel of the surface to be tested corresponds to a point
in 3D space, and this correspondence should be obvious since it's due
to the map $R^2\rightarrow R^3$. From this 3D point we need to check
whether a particular voxel is solid or not. Conceptually that sounds
easy, but it might involve a lot of calculation for every pixel.

It might be possible to speed up the ``patches with holes'' idea by
precalculating the surfaces, although I'm not sure it would be
necessary. The reason is might not be necessary is that each surface
patch should cover a relatively large area of the object, and there
shouldn't be that many patches. However, in a very complicated object,
you could have more patches, and each of these patches might have a
lot of holes -- think of something like cutting a surface that looks
like a person's face. By the time each patch has had the holes
removed, there won't be much of the surface left. There will be many
(!) of these complicated hole-filled patches. If we think of each
NURBS patch as a map $[0,1]^2\rightarrow R^3$, then what we need is a
way to specify the domain, $D$, after the holes have all been
removed. That way, when we draw the path, we only need to consider
$D\rightarrow R^3$. Hopefully, traversing D can be done more quickly
than traversing all of $[0,1]^2$.

Something that must be considered with any analytic method is making
sure that the edges of all of these patches line up correctly so that
there are no holes in the surface where they meet. The coefficients of
the Bezier curves must be floating point values, and I don't see how
one ensures that rounding errors don't lead to small gaps. I don't
know how that is handled, but it must be a common problem.

\section{April 24, 2013}

Some further throughts... I've been thinking in terms of only three
tools: endmills, ballmills and drills, where ``drill'' includes any
tool with a pointy angle on the bottom. In fact, I need to consider
center drills and (maybe) reamers too. A reamer has a small angle cut
off the bottom edge of the cutter so that the bottom is not entirely
flat. 

I see a way that it wouldn't be \emph{too} hard to allow the user to
specify any tool shape that does not allow undercutting. It's hard to
explain without pictures, but such tool profile can be thought of as a
series of simpler tools, ``grown'' one on top of the other. You ``walk
out and upward'' (never inward) from the bottom center of the tool to
trace the profile. Each of the steps is either linear or a portion of
an ellipse. In both cases, all you need is the $x$ and $y$ distance of
the step, and in the ellipse case, whether the arc is concave up or
concave down. This wouldn't be that hard to implement, but I'm not
sure if it's worth the trouble. In any case, it's the kind of thing
that could be dropped in later without causing any serious
re-engineering of the larger program.

Concerning how to render the image, I see two cases. There's the
normal case of a typical part with a relatively simple surface, and
there's the extreme case of machining something like a human face.

In the normal case, the number of triangles should be managable and
the important issue is the accuracy of representation. If the edges of
a cut, like the semicircular end of a groove cut by and endmill, are
jagged, this will be visually obvious. To get accuracy of
representation, run the DA through a filter that converts it to a
series of scanlines. Thus, each line becomes a much shorter series of
numbers, where the number only changes if the depth changes.\footnote{I think I
tried something like this before, where there was no DA at all; only
the scanlines. The problem was that adding a new cut to such a thing
is complicated. The reason the idea is attractive is that it takes
orders of magnitude less memory.} Now you have a much shorter
representation that is just as accurate as the full DA. How to
triangulate this is still open in my mind.

In the extreme case, the scanline idea has no advantage over the
original DA; it won't be any shorter, and will probably be longer due
to the extra data held with each change in depth. Here, the rendering
strategy should probably be to simply draw each voxel as a three-sided
cube. The problem is that rendering billions of cube faces isn't
feasible. But the nature of the surface means that some averaging
would be OK. Form a new depth array by averaging each five by five
(say) area of the original DA, then render this new, much smaller DA,
as a set of voxel cubes. The question is how to handle this
averaging. For something like a human face, a simple average is
probably fine. In other cases, you may want to throw out any
outliers. Maybe I should measure the dispersion and act
accordingly. For instance, if there are fewer than 5 (out of 25)
outliers, then throw them away; otherwise, do a simple average.



\end{document}
